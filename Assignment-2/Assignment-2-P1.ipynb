{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b7609a-1d43-457b-8b79-94ca513af0e9",
   "metadata": {},
   "source": [
    "# Neural Network optimization with SGD and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed025da-cb91-4a55-bd2b-2097f6ebfded",
   "metadata": {},
   "source": [
    "In this assignment you are asked to study the behavior of Adam and compare with SGD. You will be replicating the results of section 6 from the paper [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) by Diederik P. Kingma and Jimmy Ba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c710b-69f3-419d-890d-b9ca40b19276",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2d9d3-fa63-4f18-b637-40934c1be019",
   "metadata": {},
   "source": [
    "You will use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) and [CFAR10](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10) datasets and MNIST datasets. The IMDB dataset is a set of 50,000 highly polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing. The CFAR10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c7d6e2-ed4c-4a88-ab24-e593861ecf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 6s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#imdb dataset\n",
    "(x_train_imdb, y_train_imdb), (x_test_imdb, y_test_imdb) = tf.keras.datasets.imdb.load_data(\n",
    "    path='imdb.npz',\n",
    "    num_words=None,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3\n",
    ")\n",
    "\n",
    "#cfar10 dataset\n",
    "(x_train_cfar10, y_train_cfar10), (x_test_cfar10, y_test_cfar10) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#mnist dataser\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982786b-a35f-449c-92a4-d19d12fd4da2",
   "metadata": {},
   "source": [
    "## Create the BoW feature vectors  (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81740a-2841-4a62-bcdf-55bc0a6cfe43",
   "metadata": {},
   "source": [
    "Create the word vectors using Bag of Words (BoW) representation. You can use the following code to get the BoW representation of the dataset. You can read more about BoW [here](https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc862d44-7cae-478c-904c-25784368ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9010)\t3\n",
      "  (0, 3489)\t6\n",
      "  (0, 9686)\t11\n",
      "  (0, 4946)\t4\n",
      "  (0, 1238)\t3\n",
      "  (0, 1484)\t1\n",
      "  (0, 5342)\t1\n",
      "  (0, 7769)\t1\n",
      "  (0, 8565)\t2\n",
      "  (0, 2631)\t1\n",
      "  (0, 3184)\t2\n",
      "  (0, 7206)\t2\n",
      "  (0, 8703)\t1\n",
      "  (0, 8967)\t15\n",
      "  (0, 6472)\t2\n",
      "  (0, 8994)\t4\n",
      "  (0, 6693)\t2\n",
      "  (0, 473)\t9\n",
      "  (0, 9970)\t4\n",
      "  (0, 2126)\t1\n",
      "  (0, 4514)\t1\n",
      "  (0, 948)\t2\n",
      "  (0, 8988)\t2\n",
      "  (0, 7555)\t1\n",
      "  (0, 7252)\t1\n",
      "  :\t:\n",
      "  (24999, 9858)\t1\n",
      "  (24999, 3367)\t1\n",
      "  (24999, 9294)\t1\n",
      "  (24999, 4329)\t1\n",
      "  (24999, 4681)\t1\n",
      "  (24999, 9607)\t1\n",
      "  (24999, 3391)\t1\n",
      "  (24999, 559)\t1\n",
      "  (24999, 546)\t1\n",
      "  (24999, 7340)\t1\n",
      "  (24999, 9654)\t1\n",
      "  (24999, 7351)\t1\n",
      "  (24999, 1651)\t2\n",
      "  (24999, 8256)\t1\n",
      "  (24999, 5193)\t1\n",
      "  (24999, 7945)\t1\n",
      "  (24999, 4045)\t1\n",
      "  (24999, 9669)\t1\n",
      "  (24999, 3531)\t1\n",
      "  (24999, 2208)\t1\n",
      "  (24999, 7702)\t1\n",
      "  (24999, 861)\t1\n",
      "  (24999, 1395)\t1\n",
      "  (24999, 7548)\t1\n",
      "  (24999, 844)\t1   (0, 396)\t1\n",
      "  (0, 409)\t1\n",
      "  (0, 414)\t1\n",
      "  (0, 473)\t2\n",
      "  (0, 1187)\t4\n",
      "  (0, 1482)\t1\n",
      "  (0, 2126)\t1\n",
      "  (0, 2751)\t1\n",
      "  (0, 3372)\t1\n",
      "  (0, 3550)\t3\n",
      "  (0, 3888)\t2\n",
      "  (0, 3928)\t1\n",
      "  (0, 4185)\t1\n",
      "  (0, 4198)\t1\n",
      "  (0, 4306)\t2\n",
      "  (0, 4410)\t1\n",
      "  (0, 4790)\t1\n",
      "  (0, 5055)\t2\n",
      "  (0, 5464)\t2\n",
      "  (0, 5731)\t1\n",
      "  (0, 5805)\t2\n",
      "  (0, 6234)\t1\n",
      "  (0, 6274)\t1\n",
      "  (0, 6276)\t2\n",
      "  (0, 6348)\t1\n",
      "  :\t:\n",
      "  (24999, 7902)\t1\n",
      "  (24999, 8018)\t1\n",
      "  (24999, 8054)\t2\n",
      "  (24999, 8058)\t1\n",
      "  (24999, 8141)\t1\n",
      "  (24999, 8235)\t1\n",
      "  (24999, 8557)\t1\n",
      "  (24999, 8565)\t1\n",
      "  (24999, 8566)\t1\n",
      "  (24999, 8960)\t1\n",
      "  (24999, 8965)\t1\n",
      "  (24999, 8967)\t8\n",
      "  (24999, 8977)\t1\n",
      "  (24999, 8988)\t2\n",
      "  (24999, 9010)\t2\n",
      "  (24999, 9093)\t2\n",
      "  (24999, 9130)\t1\n",
      "  (24999, 9153)\t1\n",
      "  (24999, 9440)\t1\n",
      "  (24999, 9478)\t1\n",
      "  (24999, 9533)\t1\n",
      "  (24999, 9686)\t1\n",
      "  (24999, 9710)\t1\n",
      "  (24999, 9855)\t4\n",
      "  (24999, 9970)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 10000)\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def bow(index):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in index])\n",
    "\n",
    "x_train_bow = vectorizer.fit_transform([bow(index) for index in x_train_imdb])\n",
    "x_test_bow = vectorizer.transform([bow(index) for index in x_test_imdb])\n",
    "print(x_train_bow, x_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e342f0-96ad-4283-8ea8-f65a9e9e116c",
   "metadata": {},
   "source": [
    "## Implement the models (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d094e-493a-46e7-af0c-947f4bd48b64",
   "metadata": {},
   "source": [
    "You need to implement Logistioc Regression, MLP and CNN models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008e936-14c7-4945-bd2e-d893b4a8acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, optimizer, max_iter=100):\n",
    "    if optimizer == 'sgd':\n",
    "        model = LogisticRegression(solver='sgd', max_iter=max_iter)\n",
    "    elif optimizer == 'adam':\n",
    "        model = LogisticRegression(solver='adam', max_iter=max_iter)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer. Supported options: 'sgd', 'adam'\")\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(x_train, y_train)\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a6ef6-9977-4a10-a972-e78fc085e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "def mlp_model(x_train, y_train, x_test, y_test, optimizer, hidden_layers=(100,)):\n",
    "    if optimizer == 'sgd':\n",
    "        model = MLPClassifier(solver='sgd', hidden_layer_sizes=hidden_layers, max_iter=1000)\n",
    "    elif optimizer == 'adam':\n",
    "        model = MLPClassifier(solver='adam', hidden_layer_sizes=hidden_layers, max_iter=1000)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer. Supported options: 'sgd', 'adam'\")\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(x_train, y_train)\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f757b9a-2ecd-40fb-a9b5-d19210c76011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cnn_model(x_train, y_train, x_test, y_test, optimizer, epochs=10, batch_size=32):\n",
    "    input_shape = x_train.shape[1:]\n",
    "    num_classes = len(set(y_train))\n",
    "    if optimizer == 'sgd':\n",
    "        opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer. Supported options: 'sgd', 'adam'\")\n",
    "        \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb848da9-5571-45ce-91c3-79f54ca95346",
   "metadata": {},
   "source": [
    "## SGD and Adam optimizers (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88147593-83c2-4cf2-a6b4-b05b4f9898ed",
   "metadata": {},
   "source": [
    "Use SGD and Adam optimizers with Optuna to find the best hyperparameters for the models. You can read more about Optuna [here](https://optuna.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f599fcaf-4d9e-4f13-9d4d-bc0201b16bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/69/60/87a06ef66b34cbe2f2eb0ab66f003664404a7f40c21403a69fad7e28a82b/optuna-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading optuna-3.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/a2/8b/46919127496036c8e990b2b236454a0d8655fd46e1df2fd35610a9cbc842/alembic-1.12.0-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.12.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting cmaes>=0.10.0 (from optuna)\n",
      "  Obtaining dependency information for cmaes>=0.10.0 from https://files.pythonhosted.org/packages/f7/46/7d9544d453346f6c0c405916c95fdb653491ea2e9976cabb810ba2fe8cd4/cmaes-0.10.0-py3-none-any.whl.metadata\n",
      "  Downloading cmaes-0.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (23.1)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna)\n",
      "  Obtaining dependency information for sqlalchemy>=1.3.0 from https://files.pythonhosted.org/packages/54/c2/c51f040038859732f781f25907e01ee980987d24fa0747884e6073363a14/SQLAlchemy-2.0.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading SQLAlchemy-2.0.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.3.0->optuna)\n",
      "  Obtaining dependency information for greenlet!=0.4.17 from https://files.pythonhosted.org/packages/6c/df/1e3e52e35e56b912c7bcd64ba2010d6972c43dff96794074b32a62345970/greenlet-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading greenlet-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
      "Downloading SQLAlchemy-2.0.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.4/616.4 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, Mako, greenlet, colorlog, cmaes, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.12.0 cmaes-0.10.0 colorlog-6.7.0 greenlet-3.0.0 optuna-3.3.0 sqlalchemy-2.0.21 tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d304cb1-c425-4e64-bef7-d20a99e50b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    epsilon = trial.suggest_loguniform('epsilon', 1e-8, 1e-5)\n",
    "    beta_1 = trial.suggest_uniform('beta_1', 0.85, 0.99)\n",
    "    beta_2 = trial.suggest_uniform('beta_2', 0.9, 0.999)\n",
    "    model = tf.keras.Sequential()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, beta_1=beta_1, beta_2=beta_2)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train the model on the training data\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    _, accuracy = model.evaluate(x_test, y_test)\n",
    "    return -accuracy\n",
    "\n",
    "def optuna_adam(x_train, y_train, x_test, y_test):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    best_params = study.best_params\n",
    "    best_learning_rate = best_params['learning_rate']\n",
    "    best_epsilon = best_params['epsilon']\n",
    "    best_beta_1 = best_params['beta_1']\n",
    "    best_beta_2 = best_params['beta_2']\n",
    "    final_model = tf.keras.Sequential()\n",
    "    final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate, epsilon=best_epsilon, beta_1=best_beta_1, beta_2=best_beta_2)\n",
    "    final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    final_model.fit(x_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e8773-bf95-4cae-be9c-f4e743a9b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    epsilon = trial.suggest_loguniform('epsilon', 1e-8, 1e-5)\n",
    "    beta_1 = trial.suggest_uniform('beta_1', 0.85, 0.99)\n",
    "    beta_2 = trial.suggest_uniform('beta_2', 0.9, 0.999)\n",
    "    model = tf.keras.Sequential()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, epsilon=epsilon, beta_1=beta_1, beta_2=beta_2)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train the model on the training data\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    _, accuracy = model.evaluate(x_test, y_test)\n",
    "    return -accuracy\n",
    "\n",
    "def optuna_sgd(x_train, y_train, x_test, y_test):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    best_params = study.best_params\n",
    "    best_learning_rate = best_params['learning_rate']\n",
    "    best_epsilon = best_params['epsilon']\n",
    "    best_beta_1 = best_params['beta_1']\n",
    "    best_beta_2 = best_params['beta_2']\n",
    "    final_model = tf.keras.Sequential()\n",
    "    final_optimizer = tf.keras.optimizers.SGD(learning_rate=best_learning_rate, epsilon=best_epsilon, beta_1=best_beta_1, beta_2=best_beta_2)\n",
    "    final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    final_model.fit(x_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4e5b9-fbc8-4093-890b-3dba28284996",
   "metadata": {},
   "source": [
    "## Compare the results (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de1cd6-d1bb-4320-b809-5b4d7994913a",
   "metadata": {},
   "source": [
    "Comment on whether the results of the paper have been replicated and on the relative merits of Adam vs SGD.  Consider though the statements made on page 24 of [this reference](https://arxiv.org/pdf/1912.08957.pdf) and comment on how hyperparameter optimization may make the empirical results of the paper less relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
